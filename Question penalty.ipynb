{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport os, pandas, numpy, pickle\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of fqi.reward_function failed: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/extensions/autoreload.py\", line 384, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/extensions/autoreload.py\", line 323, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/IPython/extensions/autoreload.py\", line 278, in update_class\n",
      "    if old_obj == new_obj:\n",
      "ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.ensemble.forest import ExtraTreesRegressor\n",
    "\n",
    "from fqi.et_tuning import run_tuning\n",
    "from fqi.reward_function import *\n",
    "from fqi.sars_creator import to_SARS\n",
    "from fqi.utils import *\n",
    "\n",
    "from trlib.policies.valuebased import EpsilonGreedy, Softmax\n",
    "from trlib.policies.qfunction import ZeroQ\n",
    "from trlib.algorithms.reinforcement.fqi_driver import FQIDriver, DoubleFQIDriver\n",
    "from trlib.environments.trackEnv import TrackEnv\n",
    "from trlib.utilities.ActionDispatcher import *\n",
    "\n",
    "from fqi.fqi_evaluate import run_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulations = pd.read_csv('trajectory/dataset_offroad.csv')\n",
    "ref_tr = pd.read_csv('./trajectory/ref_traj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLap = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"file_name = 'model_file/penalty/penalty_offroad.pkl'\\nwith open('./' + file_name, 'rb') as pen:\\n     penalty_offroad = pickle.load(pen)\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'model_file/penalty/penalty.pkl'\n",
    "with open('./' + file_name, 'rb') as pen:\n",
    "     penalty = pickle.load(pen)\n",
    "        \n",
    "\"\"\"file_name = 'model_file/penalty/penalty_offroad.pkl'\n",
    "with open('./' + file_name, 'rb') as pen:\n",
    "     penalty_offroad = pickle.load(pen)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Temporal_projection(ref_tr, penalty=penalty, clip_range=(-np.inf, np.inf))\n",
    "rf_off = Temporal_projection(ref_tr, penalty=penalty, clip_range=(-np.inf, np.inf), offroad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'numpy.ndarray'>\n",
      "shape (849,)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./trajectory/dataset_offroad.csv',\n",
    "                              dtype={'isReference': bool, 'is_partial':bool})\n",
    "NLap = 12\n",
    "data = data[data['NLap'] == NLap]\n",
    "episodes = {}\n",
    "for e in np.unique(data['NLap']):\n",
    "    mask = data['NLap'] == e\n",
    "    lap_df = data[mask]\n",
    "    n_samples = np.count_nonzero(mask)\n",
    "    # create timestamp column\n",
    "    timestamp = pd.DataFrame({'t': np.zeros([n_samples - 1])})\n",
    "    # Create NLap column\n",
    "    nlap = pd.DataFrame({'NLap': np.ones([n_samples - 1]) * e}, dtype=int)\n",
    "    # Add state and action\n",
    "    state_action = lap_df[state_cols+action_cols].iloc[:-1].reset_index(drop=True)\n",
    "    # Add reward\n",
    "    #reward = pd.DataFrame({'r': reward_function(lap_df)})\n",
    "    r = rf_off(lap_df)\n",
    "    #print(r)\n",
    "    print('type:', type(r))\n",
    "    print('shape', r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exponential'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty.kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
