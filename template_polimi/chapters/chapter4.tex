\chapter{State of the Art}
\label{State of the Art}
\thispagestyle{empty}


The objective of this chapter is to go into detail about the key aspects of reinforcement learning, the core topic of this thesis. In particular, we will describe the concept of Decision Markov Process, the experiment environment, the concept of state, actions, and reward function. Eventually, we will give insight about the main RL algorithms focusing on the ones we used in this project.
Then, in the following section, we will show some successful application on reinforcement learning to autonomous driving, videogames, and in particular racing games.

\section{Theoretical Background}

\subsection{Reinforcement Learning}
\begin{figure}[t]
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/rl}
  \caption{The basic RL loop. An agent in a state $s_t$ takes an action $a_t$ in an environment, which takes it in a state $s_{t+1}$ and receives a reward accordingly.
   \label{fig:rl-loop}
\end{figure}
Together with Supervised Learning and Unsupervised Learning \cite{elements}, Reinforcement Learning is a branch of Machine Learning, the discipline which studies the way a computer program can learn from experience and improve its performance at a specified task. Whereas Supervised Learning algorithms are based on inductive inference where the model is tipically trained using labelled data to perform classification or regression, and Unsupervised Learning exploits techniques of density estimation and clustering applied to unlabelled data, in the Reinforcement Learning paradigm an autonomous agent learns to improve its performance at an assigned task by interacting with its environment.

Tipically, Reinforcement Learning agents are not explicitly taught how to act by an expert. Instead, the agent is free to explore the environment in which it lives, and its performance is evaluated by a reward function. Its goal is to maximize the reward by acting accordingly. The resulting reward results from the sum of the reward gained at every single step in its exploration process: for each state, the agent chooses an action to take and receives a reward based on the usefulness of its decision (see Figure~\ref{fig:rl-loop}). Eventually, the agent learns the way to obtain the highest reward by exploiting knowledge learned about the expected utility of different state-action pairs. The challenge of Reinforcement Learning is to design the best tradeoff between exploration and exploitation, that is the capability of an agent of use its knowledge to obtain high rewards and, by contrast, being capable of exploring new possibilities in such a way not to remain stuck in a local optimum. Intuitively, the exploration process is high at the first iterations of the learning process, while it should decrease with accumulating knowledge.


\subsubsection{Markov Decision Process}
The Reinforcement Learning problem can be formalized as a Markov Decision Process. A MDP is a tuple composed by:
\begin{itemize}
  \item A finite set of states \(D\): it encompasses every possible state of the process;
  \item A finite set of actions \(A\): it represents the possible action the agent can take at any moment;
  	\item A reward function \(r = \psi(s_t,a_t,s_{t+1})\), which represents the reward given to the agent at state \(s_t\) taking the action \(a_t\) landing in the state \(s_{t+1};\)
  	\item A transition probability model \(T(s_t,a_t,s_{t+1}) = p(s_{t+1}|s_t,a_t)\), which indicates the probability of landing in a state \(s_{t+1}\) being in a state \(s_t\) taking an action \(a_t.\)
\end{itemize}

This provides the framework in which Reinforcement Learning operates: its goal is to find a policy \(\pi\) which specifies the actions to take in each state in order to maximize the reward over time i.e. fulfill the task.
Reinforcement learning can solve Markov decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration. In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states.
Reinforcement Learning operates under the Markov Assumption, under which the current state represents all the information needed to take an action, regardless from the past states and actions.
In other words: "The future is independent of the past given the present"

\subsubsection{Environment}
The environment is the mathematical representation of the world in which the agent operates. It reflects the set of features of a machine learning problem, which could be numerical (such as e.g. coordinates, velocities, acceleration) or raw (such as images or signals). On the representation of the environment depends the choice of the algorithm suitable to perform a learning process.

\subsubsection{Reward}
At each timestep, the agent receives a reward by the reinforcement learning algorithm, accordingly to the usefulness of action taken.
At the end of an episode (a finite sequence of states) the return is tipically computed as \[R_t = \sum^{T}_{k=0}\gamma^k r_t + k +1\] where \(\gamma\) represents a discount factor comprised between 0 and 1, which stands for a learning paramenter that influences how the agent considers future                              or present reward. A small \(\gamma\) results in a higher consideration of present rewards than the future (often caled "myopic" evaluation), while with a \(\gamma\) near to 1, the agent will consider equally the reward coming from the present either from the future (also called "far-sighted" evaluation). The role of the discount factor resides in the fact that, besides being mathematically convenient, it avoids infinite returns in cyclic Markov Process, and it's capable of addressing a non-fully represented uncertainty about the future.

\subsubsection{Policy}
The goal of reinforcement learning is finding a policy \(\pi\), which maps states to a probability distribution over the actions, in order to maximize the return. This policy is called optimal policy \(\pi^*\).
If from a state \(s_t\) the agent takes always the same action \(a_t\), the policy is deterministic:
\(\pi(s_t) = a_t\)
Otherwise, if the taking of an action rather than another is due to a probability distribution, the policy is called stochastic:
\(\pi(a_t|s_t) = p_i, 0 \leq p_i \leq 1\)


\subsubsection{Policy Evaluation}
$V\pi(s)$ is defined as the expected long-term return of the current state under policy $\pi$. 
\[V^\pi(s) = \mathbb{E}_\pi{\left[R_t|s_t=s\right]}= \mathbb{E}_\pi\left[\sum^{\infty}_{i=0}\gamma^ir_{t+i+1|s_t=s}\right] = \mathbb{E}_\pi\left[r_{t+1} + \sum^{\infty}_{i=0}\gamma^ir_{t+i+2|s_t=s}\right],\]
from which one can derive the recursive so-called Bellman Equation:

\[V^\pi(s) = \sum_a\pi(a|s)\sum_{s_{t+1}}p(s_{t+1}|s_t,a)[r(s,a,s{t+1})+\gamma V^\pi(s_{t+1})].\]

Analog to the value functions is the concept of action-value function, or Q-value, which is the expected return after taking an action \(a_t\) in state \(s_t\) and thereafter following policy \(\pi\):
\[Q^\pi(s_t,a_t)=\mathbb{E}_{r_{i \geq t},s_{i>t} \sim ,a_{i>t} \sim \pi}[R_t|s_t,a_t]\]
and the relative Bellman Equation:
\[Q^\pi(s_t,a_t)=\mathbb{E}_{r_t,s_{t+1} \sim E }\left[ r(s_t,a_t) + \gamma \mathbb{E}_{a_{t+1} \sim \pi}\left[Q^\pi (s_{t+1},a_{t+1})\right]\right]\]


\subsubsection{Model-based vs Model-free}
In practical situations, interacting with the real environment could be limited due to many reasons including safety and cost. Learning a model for environment dynamics may reduce the amount of interactions required with the real environment. Moreover, exploration can be performed on the learned models.
Model-based reinforcement learning has an agent try to understand the world and create a model to represent it. Here the model is trying to capture two functions, the transition function from states $T$ and the reward function $R$. From this model, the agent has a reference and can plan accordingly.
By contrast, in model-free approaches such knowledge is not a requirement. Instead, model-free learners sample the underlying MDP directly in order to gain knowledge about the unknown model, in the form of value function estimates for example. 


\subsubsection{On-Policy vs Off-Policy}

Learning algorithms can be on-policy or off-policy: in the former case, the policy is updated while using the same policy for control. In the latter, the trajectory through which the policy is updated is created by another policy, that could be generated by an older version of the policy or provided by an expert.
Off-policy methods, use two policies: the behavior policy, that is the policy used to generate behavior, and the target policy, the one being improved on. An advantage of this separation is that the target policy may be deterministic, while the behavior policy can continue to sample all possible actions.

\subsubsection{Offline vs Online}

Another distinction can be made between offline and online algorithms. In offline algorithms, after collecting some samples, the policy is evaluated without the agent having further interaction with the environment. In other words, the data are given and fixed. In offline algorithms, on the contrary, the agent keeps interacting with the environment,

\subsubsection{Function Approximation}
The simplest way to store learned values, policies and models is by tabular representation. However, in a complex environment, the number of state-action pair values that must be stored is likely to be vast, leading to an un acceptable amount of time needed to scan them. This problem is commonly refferred to as the "curse of dimensionality". Many real-world domains involve continuous state and/or action spaces, that in some cases can be discretized, but is often impractical or unfeasible. Another possible approach is to adopt a function approximator, that can be used to generalise across states and/or actions, whereby a function approximator is used to store and retrieve estimates.
Neural networks are typically adopted to this end, and this is commonly known as deep reinforcement learning.
A neural network can be used to approximate a value function, or a policy function. That is, neural nets can learn to map states to values, or state-action pairs to Q values. Rather than use a lookup table to store, index and update all possible states and their values, which impossible with very large problems, we can train a neural network on samples from the state or action space to learn to predict how valuable those are relative to our target in reinforcement learning.
Like all neural networks, they use coefficients to approximate the function relating inputs to outputs, and their learning consists to finding the right coefficients, or weights, by iteratively adjusting those weights along gradients that promise less error.


\subsection{Value-Based Methods}
A first category of RL algorithms are Value-Based Methods. Their goal is to find the best policy $\pi*$ by optimizing the Q-value function $Q*(s,a)$ . In this branch we can find Monte Carlo methods \cite{mc}, Dynamic Programming \cite{dp} and Temporal Difference Learning (TD) \cite{td}.
In particular, TD algorithms are able to learn from raw experiences without the need for a model of the environment. They combine ideas form dynamic-programming and from Monte Carlo methods but while Monte Carlo algorithms need to reach the end of a learning episode to update the value function TD is able to update it at the end of each step.
At each step, the value function is computed with some estimations based on the already learned ones.

\subsubsection{Q-Learning}
Q-learning is one of the most commonly used RL algorithms. It is a model-free off-policy TD algorithm that learns estimates of the utility of individual state-action pairs. Q-learning has been shown to converge to the optimum state-action values for a MDP with probability 1, so long as all the actions in all states are sampled infinitely. In practice, Q-learning will near near optimal state-action values provided a sufficient number of samples are obtained for each state-action pair. If a Q-learning agent has converged to the optimal Q values for a MDP and selects actions greediliy discounted rewards as calculated by the value function with $\pi*$. Agets implementing Q-learning update their Q values according to the following update rule: 
\[Q(s_t,a_t)=Q(s_t,a_t)+\alpha(r_{t+1}+\gamma max_aQ(s_{t+1},a_t)-Q(s_t,a_t))\]

\subsubsection{SARSA}
SARSA (State-Action-Reward-State-Action) is a model-free, on-policy TD algorithm.
A SARSA agent interacts with the environment and updates the policy based on actions taken, hence this is known as an on-policy learning algorithm. The Q-value for a state-action is updated by an error, adjusted by the learning rate $\alpha$. Q values represent the possible reward received in the next time step for taking action $a$ in state $s$, plus the discounted future reward received from the next state-action observation.
Agets implementing SARSA update their Q-values according to the following update rule:
\[Q(s_t,a_t)=Q(s_t,a_t)+\alpha(r_{t+1}+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))\]




\subsubsection{Deep Q-Network}
Q-learning and SARSA (State-Action-Reward-State-Action) are two commonly used model-free RL algorithms. They differ in terms of their exploration strategies while their exploitation strategies are similar. While Q-learning is an off-policy method in which the agent learns the value based on action $a*$ derived from the another policy, SARSA is an on-policy method where it learns the value based on its current action aderived from its current policy. These two methods are simple to implement but lack generality as they do not have the ability to estimate values for unseen states.
This can be overcome by more advanced algorithms such as Deep Q-Networks which use Neural Networks to estimate Q-values. 
Deep Q-Networks (DQN) [22] incorporates a variant of the Q-learning algorithm, by using deep neural networks as a non-linear Q function approximator over high-dimensional state spaces. In practics, the neural network predicts the value of all actions without the use of any explicit domain-specific information or hand-designed features. DQN applies experience replay technique to break the correlation between successive experience samples and also for better sample efficiency. For increased stability, two networks are used where the parameters of the target network for DQN are fixed for a number of iterations while updating the parameters of the online network. 

\subsubsection{Double Deep Q-Newtork}


\subsubsection{Fitted Q-Iteration}

The fitted Q iteration algorithm is a batch mode reinforcement learning algorithm which yields
an approximation of the Q-function corresponding to an infinite horizon optimal control problem
with discounted rewards, by iteratively extending the optimization horizon



\subsection{Policy Search}
\subsection{Policy-Based Methods}

Another approach to tackle RL problems is what is called Policy-Based Methods (also known as Policy Search, or Policy Optimization). These methods consist in optimizing a parametrized policy, instead of optimizing the Q-Value. Here we consider a parametrized policy $\pi_\theta$, belonging to a parametric policy space \(\Pi_\theta = \{\pi_\theta:\theta \in \Theta \subseteq \mathbb{R}^p\}\). The expected return can be expressed as expectation over the policy parameter space $\Theta$: \[\] \[ J_D(\rho) = \int_{\Theta} \int_{\mathcal{T}} \nu_\rho(\theta)p(\tau|\theta)R(\tau)d\tau d\theta, \] where $p(\tau|\theta)$ is the trajectory density function, or over the trajectory space $\mathcal{T}$: \[J_D(\theta) = \int_{\mathcal{T}} p(\tau|\theta)R(\tau)d\tau.\]
We would like to optimize the policy by gradient ascent:
\[\theta_{k+1} = \theta_k + \alpha \left. \nabla_{\theta} J_D(\theta) \right|_{\theta_k}.\]
The gradient of policy performance,\(\nabla_{\theta}J(\pi_{\theta}),\) is called the policy gradient.



\subsubsection{REINFORCE}



{Actor-Critic Methods}
The "Critic" estimates the value function. This could be the action-value (the Q value) or state-value (the V value).
The "Actor" updates the policy distribution in the direction suggested by the Critic (such as with policy gradients).
and both the Critic and Actor functions are parameterized with neural networks. In the derivation above, the Critic neural network parameterizes the Q value - so, it is called Q Actor Critic.

\subsubsection{Deep Deterministic Policy Gradient}
\label{ddpg}


\begin{figure}[t]
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/ac}
  \caption{The framework of Reinforcement Learning, Imitation Learning, and their integration. The demonstrations of human experts can be used to pretrain a model for RL, to shape the reward/policy of RL, and so on.}
   \label{fig:cinesi}
\end{figure}


But DQNs can only handle discrete, low-dimensional action spaces. DDPG(Deep Deterministic Policy Gradient)is a model-free, off-policy, actor-critic algorithm that tackles this problem by learning policies in high dimensional, continuous action spaces.

It is not possible to straightforwardly apply Q-learning to continuous action spaces, because in continuous spaces finding the greedy policy requires an optimization of a t at every timestep; this optimization is too slow to be practical with large, unconstrained function approximators and nontrivial action spaces. Instead, here we used an actor-critic approach based on the DPG algorithm.


Deep Deterministic Policy Gradient (DDPG), actor-critic algorithm that can learn policies for
continuous action spaces using deep neural net based function
approximation, extending prior work on DPG to large and
high-dimensional state-action spaces. When selecting actions,
exploration is performed by adding noise to the actor policy.
Like DQN, to stabilise learning a replay buffer is used to
minimize data correlation. A separate actor-critic specific
target network is also used.
By combinging the insights of DQN with the actor-critic deterministic policy gradient algorithms, DDPG allows for solving a wide variety of continuous control tasks. DDPG utilizes an actor function $\mu(s|\theta^\mu)$, specifying the current policy, and a critic function $Q(s,a|\theta^Q),$ both approximated by neural networks. At each step, based on the current state $s_t$, the agent chooses an action according to $a_t=\mu(s_t|\theta^\mu) + \mathcal{N},$ with a noise process $\mathcal{N}$ to allow for exploration, and obtains a reward $r_t$ and a new state $s_{t+1}$ from the environment. The observed transitions ($s_t,a_t,t_t,s_{t+1}$) are stored in a replay buffer. At each step, a minibatch of $N$ transitions is uniformely sampled from the buffer. The parameters of the critic network are then optimized using Adam optimization to minimize the loss given as: \[L(\theta^Q) = \frac{1}{N}\sum^N_{i=1}(y_i-Q(s_i,a_i|\theta^Q))^2\]
\[y_i=r_i+\gamma Q'(s_{i+1}|\theta^{\mu'}|\theta^{Q'})\]
where $y_i$ is the one-step target with the discount factor $\gamma$. Here, $Q'(s,a|\theta^{Q'})$ and $\mu'(s|\theta^{\mu'})$ are target networkds associated with $Q(s,a|\theta^{\theta^Q})$ and $\mu(s|\theta^\mu).$ Their parameters are updated at each step using soft updates, i.e. \(\theta' \leftarrow \tau theta + (1-\tau)\theta'\) with $\tau \ll 1.$ To update the parameters of the actor network, a step proportional to the sampled gradient of the critic with respect to the action is taken, which is given by: \[\nabla_{\theta^\mu}J\approx\frac{1}{N}\sum^N_{i=1}\nabla_aQ(s,a|\theta^Q)|_{s=s_i,a=\mu(s_i)}\nabla_{\theta_\mu}\mu(s|\theta^\mu)|_{s=s_i}.\]

\subsubsection{Policy Optimization via Importance Sampling}

Policy Optimization via Importance Sampling (POIS) \cite{pois} is a model-free actort-only policy optimization algorithm which tackles the problem of finding the best policy in a parametric space by mixing online and offline optimization to efficiently eploit the information contained in the collected trajectories. POIS takes into account the uncertainty introduced  by the importance sampling by optimizing a surrogate objective functions. 
A distinction can be made between methods whose goal is to optimize the policy gradient and the ones that aim at finding parameters of the best hyperpolicy.
In action-based methods, the learning agent performs a search in a parametric policy space by following the gradient of the utility function estimated by means of a batch of trajectories collected from the environment.
Its goal is to find the policy parameters \(\theta*\) that maximize \(J_D(\theta)\). In such methods, the exploration tipically needs to be enforced by means of the stochasticiy of $\pi_\theta$.
In parameter-based methods, the agent searches in the space of parameters by exploiting global optimizers or following a proper gradient direction. The agent is equipped with a hyperpolicy $\nu$ belonging to a hyperpolicy space \(\mathcal{N}_\mathcal{P} = \{\nu_\rho:\rho \in \mathcal{P} \subseteq \mathbb{R}^r\}\) used to sample the policy parameters at the beginning of each episode. The agent's goal is to determine the hyperparameters \(\rho*\) so as to maximize \(J_D(\rho)\). In such cases, the stochasticiy of the hyperpolicy is a sufficient source of exploration.
There are two versions of the algorithm: Action-based POIS (A-POIS), which is based on a policy gradient approach, and Parameter-based POIS (P-POIS), which adopts the PGPE framework.

In A-POIS, the goal is to find a policy that maximizes the performance index $J_D(\theta)\) within a parametric space \(\Pi_\Theta = \{ \pi_\theta:\theta \in \Theta \subseteq \mathbb{R}^p\}\) of stochastic differentiable policies.
The loss function cannot be directly optimized via gradient ascent since computing \(d_\alpha (p(\cdot|\theta')\|p(\cdot|\theta))\) requires the approximation of an integral over the trajectory space, and the transition model $P$ is unknown in a model-free setting, 
Hence, a surrogate objective can be formulated as: \[\mathcal{L}_\lambda^{\text{A-POIS}}(\theta'/\theta)=\frac{1}{N} \sum^N_{i=1}w_{\theta'/\theta}(\tau_i)R(\tau_i)-\lambda\sqrt{\frac{\hat{d_2}(p(\cdot|\theta')\|p(\cdot|\theta))}{N}},\] where \[w_{\theta'/\theta}(\tau_i)=\frac{p(\tau_i|\theta')}{p(\tau_i|\theta)}=\prod^{H-1}_{t=0}\frac{\pi_{\theta'}(a_{\tau_i,t}|s_{\tau_i,t})}{\pi_{\theta}(a_{\tau_i,t}|s_{\tau_i,t})}.\]
\(\pi(\cdot|s)\) is assumed to be a Gaussian distribution over actions whose mean depends on the state and whose covariance is state-independent and diagonal: \(\mathcal{N}(u_\mu(s),diag(\sigma^2))\), where $\theta=(\mu,\sigma)$.
The learning process mixes online and offline optimization. At each online iteration $j$, a dataset of $N$ trajectories is collected by executing in the environment the current policy $\pi_{\theta_{0}^j}$. These trajectories are used to optimize the loss function \(\mathcal{L}^{\text{A-POIS}}_\lambda\). At each offline iteration $k$, the parameters are update via gradient ascent \[\theta^j_{k+1}=\theta^j_k+\alpha_k\mathcal{G}(\theta^j_k)^{-1}\nabla_{\theta^j_k}\mathcal{L}(\theta^j_k/\theta^j_0)\] where $\alpha_k > 0$ is the step size and $\mathcal{G}(\theta^j_k)$ is a positive semi-definite matrix.


In P-POIS, the goal is to learn the hyperparameters $\rho$ so as to maximize $J_D(\rho)$ within the parametrized policy space $\Pi_\Theta=\{\pi_\theta:\theta \in \Theta \subseteq \mathbb{R}^p\}$, with $\pi_\theta$ not differentiable. Here, the policy parameters $\theta$ are sampled at the beginning of each episode from a parametric hyperpolicy $\nu_\rho$ selected in a parametric space \(\mathcal{N}_\mathcal{P} = \{\nu_\rho:\rho \in \mathcal{P} \subseteq \mathbb{R}^r\}\).
To this end, a Gaussian hyperpolicy $\nu_\rho$ with diagonal covariance matrix is used, $\mathcal{N}(\mu,diag(\sigma^2))$ with $\rho=(\mu,\sigma)$.
A surrogate objective can be formulated as: \[\mathcal{L}_\lambda^{\text{P-POIS}}(\rho'/\rho)=\frac{1}{N} \sum^N_{i=1}w_{\rho'/\rho}(\theta_i)R(\tau_i)-\lambda\sqrt{\frac{d_2(\nu_{\rho'}\|\nu_\rho)}{N}},\] where \[w_{\rho'/\rho}(\theta)=\frac{\nu_{\rho'}(\theta)p(\tau|\theta)}{\nu_rho(\theta)p(\tau|\theta)}=\frac{\nu_{\rho'}(\theta)}{\nu_\rho(\theta)}.\]
Each trajectory $\tau_i$ is obtained by running an episode with action policy $\pi_{theta_i}$, and the corrisponding policy parameters $\theta_i$ are sampled independently from hyperpolicy $\nu_\rho$ at the beginning of each epidose. The hyperpolicy parameters are then updated offline as: \[\rho^j_{k+1}=\rho^j_k+\alpha_k\mathcal{G}(\rho_k^j)^{-1}\nabla_{p_k^j}\mathcal{L}(\rho^j_k/\rho^j_0)\]




\subsection{Imitation Learning}
\begin{figure}[t]
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/cinesi}
  \caption{From \cite{cinesi}. The framework of Reinforcement Learning, Imitation Learning, and their integration. The demonstrations of human experts can be used to pretrain a model for RL, to shape the reward/policy of RL, and so on.}
   \label{fig:cinesi}
\end{figure}


\subsubsection{Teacher-Student Interaction for Behavioral Cloning}
\label{teacher-student}

\cite{il}


\section{Related Works}



In this section we will describe some of the applications of Reinforcement Learning, in the context of videogames, autonomous driving and racing. 


\subsection{Reinforcement Learning in Videogames}

All the Reinforcement Learning algorithms require interaction with the environment to learn a policy from the gathered experience. Trajectories can be collected by using the real-world environment or through a simulator. 
Both approaches have pros and cons, and the choice mainly depends on the context and the application of the learning agent. 

Using the physical environment would be desirable, because it is where the agent acts once it learns the target behaviour, but this is not always possible: sometimes it is not accessible, it is expensive to access to, or it is dangerous to do. In such cases, we make use of a simulator. 
The drawbacks of learning in a physical system reside mainly in the fact that exploration is expensive and constrained to states that are safe and reachable. A learning agent is often a robot, composed of mechanical parts, which may perform a range of movements that can be harmful - to itself, to other people, to the environment itself - if not behaving in an intended manner. Think about a car hitting a wall with a person aboard. Simulators, on the other hand, are safe: everything is happening inside a computer, in a virtual world, and everything - or almost - can be properly tuned before putting the code on the physical robot. Another downside of learning in the real world is expensiveness: machine learning, relying on statistics, requires vast amount of data to be realiable, which in an RL context translates into a large number of repetitions of an experiment. In a physical environment the components are typically more expensive than on a simulator, which requires AC current to power one or more machines. A robot requires one or more engines, which can be electrical (consuming much more than computers) or combustion-like, which require some kind of fuel, like gasoline. Moreover, wear comes into play, which requires fixing, or replacing, for instance the wear of tyres due by friction with asphalt.  Moreover, possible repaires are required when the robot crashes or hurts itself or the environment must be taken into account. 
Another advantage in using a simulated environment is the simplicity of its usage: in a real-world scenario, one would need to deal with sensors, data processing, and phyisical actuators, which might require spending a vast amount of time in managing hardware and software aspects (e.g. hardware requirements, input-output compatibility between sensors, processing unit and actuators, errors and unvertainties of measures). Moreover, some simulators provide high-level data, which makes it possible to skip the information extraction stages, that could be challenging. For instance, The Open Racing Car Simulator (TORCS) provides telemetry data for the cars, including speed and angle of steering, which otherwise should be computed with some kind of image analysis, such as image segmentation via supervised learning through a neural network that is not a trivial task at all.
Whether simulators may be preferred for the reasons listed above, they have drawbacks too. A simulated environment must replicate the world and an agent to a certain degree of accuracy: replicating a perfect copy would be unfeasible, because of the power of computation required to run it, and the intrinsic complexity of the rules which govern it, which today are known to a certain extent. On the other side, a simple representation of the world is much leaner to run and simpler to be coded, but may be undermodeling with respect to the task. The main difficulty here is finding is the best trade off.
Another aspect to take into consideration when choosing to learn on a simulated environment is that the computed parameters of the equations ruling the world and the actions to fulfill a task may be different compared to the real ones, thus they may need some fine tuning when embodied into the machine. This is due to the approximations and all the aspects which are not taken into account by the model of the simulator. For instance, wear of components and deformation dynamics are ruled by complex models which are hard to replicate on simulators.
As said, the choice between real environment and a simulated one mainly depends on the context and the application of the learning agent. When exploration is not dangerous, real world environments are preferred whereas simulator must be used when the real agent can hurt itself or damage environment.
RL is particularly suitable to contexts that allow performing many tries of a particular task, because RL algorithms enable an agent to learn from its experience by giving a representation of the environment without an apriori knowledge of its dynamics. For this reason, videogames and computer simulators are suitable to run a reinforcement learning algorithm on.
A notorius videogame played by a RL autonomous agent is Atari \cite{atari} by Google's Deepmind. By learning a deep convolutional neural network to approximate the Q-function, Mnih et al. successfully construct a Deep Reinforcement Learning framework called Deep Q-Network (DQN) which plays Atari games at human level. It takes as representation of the world, the raw pixels, and learns an end-to-end policy which maximizes the q-values.
Another famous example is Alphago also by Deepmind \cite{alphago}, which beated for several years consecutively the human champion at Go game. This is not a videogame but a board game, however the case is noteworthy, because such game is known for being one of the most complex  for his huge set of rules, which makes brute-force approaches infeasible. To achieve this result, Alphago has been trained with a supervised learning neural network from past experience, and then, a reinforcement learning algorithm has been applied to try and beat itself's own play. 
In racing games, which is the scope of our thesis, different solutions have been proposed over the years. In the next section, we will introduce the concept of autonomous driving and in particular in the racing context. We will expose the problem of finding the optimal trajectory and explore some of the state-of-the-art techniques which tackle this issue.
In our thesis we chose to use a simulator and in particular we worked with TORCS.

\subsection{Autonomous Driving in Videogames}
Finding a racing line that allows achieving a competitive lap-time is a key problem in real-world car racing as well as in the development of non-player characters for a commercial racing game.
The optimal racing line is defined as the line to follow to achieve the best lap-time possible on a given track with a given car. As the lap-time depends both on the distance raced and on the average racing speed, finding the optimal racing involves two different sub-problems: racing the shortest distance possible and racing as fast as possible along the track.

Over the years, different attempts to achieve time-optimal racing have been made, evolving together with technology. Besides Reinforcement Learning, controlling a self-driving car can be done with a planner - if the optimal trajectory is computed apriori - or by a controller. In this section, we provide an overview of such techniques. 


Botta et al. \cite{botta} show how to encode a racing line by a set of connected B\'{e}zier curves, such that each one defines a small portion of the racing line. B\'{e}zier curves are a family of parametrized curves widely used in computer graphics and in related fields. Initially used for drawing cars, nowadays B\'{e}zier curves are frequently used in vector graphics to model smooth paths, as they offer a compact and convenient representation.
A B\'{e}zier curve is defined by a set of control points, with the first and the last ones being respectively the beginning and the end of the curve, while the intermediate control points do not usually lie on it. Therefore, the evolution is responsible of the entire design of the racing line. Figure~\ref{fig:bezier} shows an example of Bézier curve defined by
a set of 9 control points.
  \begin{figure}
    \centering
 	  \captionsetup{width=10cm}
      \includegraphics[width=10cm]{./img/bezier}
     \caption{From \cite{botta}. An example of B\'{e}zier curve: black points are the control point of the curve; blue lines simply connect with straight segments the control points; red line is the resulting B\'{e}zier curve.}
   \label{fig:bezier}
  \end{figure}
In addition, the authors compare two different methods to evaluate the evolved racing line; the first one is based on testing the evolved racing lines in a racing simulator; the second one consists of estimating the performance of a racing line through a computational model.
The former consists in making a TORCS predefined controller follow a determined trajectory. Then, a fitness function is computed as the lap-time achieved during the best lap. While this approach does not require any previous domain knowledge, as the fitness is the result of simulation, it is also rather expensive in computational terms as it requires a full simulation. 
On the contrary, the latter relies on a computational model which provides an estimate of the lap-time
that could be achieved following the racing line to evaluate. In particular, as soon as an estimate of the lap-time is available, the fitness function is computed as in the simulation-based method. This way, the computation is generally significantly less expensive than the simulation-based evaluation. As a drawback, a previous domain knowledge is required in order to create a model able to estimate the speed that can be reached in each point of the racing line. Moreover, the accuracy of the estimation depends on the precision of the model itself.

Another approach has been proposed in \cite{ahura}, where the authors implemented a controller called Ahura for TORCS based on heuristics.
The controller uses five modules: 
\begin{itemize}
\item Steer controller: This module uses the estimated angle
of the turn in front and the vacant distance in front to determine the steer angle. The module can control how smooth or sharp the vehicle is going to turn.
The main idea behind the calculation of the steer angle is to find the proximity sensor that has the maximum empty space in front (called the base sensor). The angle of the base sensor, together with some other auxiliary proximity sensors (as shown in Figure~\ref{fig:ahura-steer}), is then used to set the angle of the steer.
\begin{figure}
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/ahura-steer}
  \caption{From \cite{ahura}. Red vector, the yellow vector, green vectors, and the blue vector represent the zero sensor, the base sensor, auxiliary sensors, and the final angle to take the turn, respectively.}
   \label{fig:ahura-steer}
\end{figure}
\item Speed controller: The aim of the speed controller is to determine the speed of the vehicle according to the current situation without considering opponents. This decision then is translated to the acceleration/breaking pedals. This module uses the estimated turn angle together with the vacant distance in front to decide the safe speed.
The target speed is mapped with a nonlinear function to values of acceleration and brake. Then, it is necessary to make sure that these values are applied in an appropriate way.  During the brake, the spin of the wheels might become smaller than the speed of the vehicle: that means that the wheels are slipping. Also, during the acceleration, the spin of the wheels might increase more than the speed of the vehicle: that means the vehicle is in traction. The ABS and ASR technologies solve these issues through keeping the speed of the spinning wheels and the speed of vehicle as close as possible.
Moreover, a gearing system is implemented, based on the minimum and maximum values of the rpm for each gear;
\item Opponent manager: This module creates a map of opponents around and finds the vacant slot to overtake. This action may entail modification of the speed and steer calculated by the speed and steer controller modules.
Ahura’s opponent manager contains two modules, steer reviser and speed reviser, that are responsible to revise steer and speed for overtaking. The information about opponents provided by opponent's sensors contain their distance and angle from the current position of the vehicle. This means that the position of opponents is provided in a polar coordinates system with the center of the measuring the vehicle. Ahura builds a spatial map of the position of opponents, which is used to revise steer and speed of the vehicle for overtaking purposes;
\item Dynamic adjuster: This module uses the mechanical specifications of the track (friction, bumps) as well as recorded difficulties the controller has experienced during the earlier laps and adjusts the current driving style.
The dynamics parameters are estimated using an optimization-based approach to find the best values so that Ahura can drive a specific bot in TORCS. There are 23 parameters in Ahura that need to be determined: eight parameters for the steer controller, ten parameters for the speed controller, five parameters for the opponent manager.
They were determined by using the optimization algorithm CMA-ES \cite{cmaes}, which has a good performance in continuous space, works with nonlinear systems, no constraint handling technique is required, and it is appropriate for nonseparable search spaces;
\item Stuck manager: This module controls the vehicle when it is out of the track or it has stuck somewhere.
The calculated parameters for the speed and steer controllers need to be revised based on the specifications of the track. The reason is that the parameters have been set for a limited number of tracks while new tracks might have different specifications. The main characteristics of tracks that may affect the best choice for parameters are friction, width, and bumps. Also, Ahura is able to handle stuck situation. In fact, if Ahura recognizes that the car is off the track, it reduces the maximum value of the acceleration pedal to prevent too much traction. It finds the correct direction first and tries to get back to the track. If it detects that the car is not moving, the gear is changed to -1 (rear gear) and the steer is adjusted accordingly to repair the direction of movement.
\end{itemize}


Existing advanced control based attempts to minimum-time driving usually provide only offline open-loop solutions.
In \cite{mpc}, the authors experimentally compared different time-optimal nonlinear MPC \cite{mpc_orig} formulations based on least squares objectives with an economic cost function in a real-time setup of small-scale model race cars, with the help of ACADO simulation toolkit from MATLAB \cite{acado}.
MPC, standing for Model Predictive Control is a multivariable control algorithm that uses an internal dynamic model of the process, a cost function over the receding horizon and an optimization algorithm minimizing the cost function using the control input.
The tight real-time bounds imposed on computational times make it necessary to reformulate the problem so as to allow for the use of efficient algorithms. To this end, they employed a nonlinear bicycle model \cite{bycicle} (see Figure~\ref{fig:bycicle}) in a reformulation to spatial coordinates and proposed an infeasible-time-tracking objective for the best practical results. In the literature, bycicle models are often adopted because, compared to higher fidelity vehicle models, system identification is easier, being there only two parameters to identify, $l_f$ and $l_r$.
For an approximate solution of the time-optimal driving problem, the aim is to minimize the time required for the race car to reach the end of the fixed-length spatial prediction horizon. For prediction horizons which tend to infinity this objective tends to the goal of driving time-optimally. As a consequence, long horizons are expected to yield a good approximation of the original problem in practice.
In the author's formulation, by providing a suffciently small (i.e., infeasible) "target time" \(T_{ref}\) they can have an approximate time-optimal MPC formulation in least-squares form. The performance of the car with this formulation is shown in Figure~\ref{fig:mpc-comparison} for several laps, where it can be seen that the cost
function allows the car to deviate from the centerline in order to minimize the time tracking error.
The real-world experiments showed that this approach has potential, but the bicycle model proved insufficient at high velocities due to slip effects not being modelled.
\begin{figure}
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/bycicle}
  \caption{From \cite{bycicle}. Kinematic Bicycle Model: $x$ and $y$ are the coordinates of the center of mass in an inertial frame ($X$, $Y$). \(\psi\) is the inertial heading and $v$ is the speed of the vehicle. $l_f$ and $l_r$ represent the distance from the center of the mass of the vehicle to the front and rear axles, respectively. $\beta$ is the angle of the current velocity of the center of mass with respect to the longitudinal axis of the car. $\delta_f$ and $\delta_r$ are respectively the front and rear steering. Since in most vehicles the rear wheels cannot be steered, tipically $\delta_r$ is assumed equal to $0$.}
  \label{fig:bycicle}
\end{figure}
\begin{figure}
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/mpc-comparison}
  \caption{From \cite{mpc}. Comparison of the performance of trajectory tracking and time-optimal driving on the experimental setup.}
	 \label{fig:mpc-comparison}
\end{figure}

\subsection{Reinforcement Learning in Racing Videogames}

With the evolution of technologies and algorithms, the focus has been shifting towards reinforcement learning and, in general, data-driven algorithms, which are achieving promising results in many applications, including autonomous driving and racing. 

In \cite{formularl} the authors tackle the problem by exploring different formulations of the DDPG algorithm \cite{ddpg} in TORCS.
Here, telemetry data are used in order to make an agent learn to drive and outperform the standard bots included in the simulator.
To achieve that, they provide the critic network of DDPG with a Long Short Term Memory \cite{lstm}, making the network recurrent, and enabling the agent to exploit the knowledge of the past. To this end, they also make the algorithm consider a window of multiple states and multiple reward instead of single ones.
While in the standard algorithm a one-step target is used for updating the critic function, they adopt a multi-step targets strategy, which incorporate the next $n$ rewards obtained along the trajectory starting from state $s_t$ and following a policy close to the current policy \(\pi\) at time step $t$.
Concerning the multiple states strategy, called Window sampling, the intuition is that in partially observable environments, accessing a single state does not reveal the full underlying state of the environment at each time step. Window sampling provides the agent additional information by feeding a window of the last $w$ states to the actor and the critic network.
Finally, they substitute the uniform distribution used by DDPG to sample the transition from the replay buffer with a Prioritized Experience Replay, which attempts to make learning more efficient by sampling more frequently transitions that are more important for learning.
Their study was split in three parts, by training three different models, in order to enhance the generalization capability. The first one uses a simple track and trained for $500$ episodes. The learned model was tested without exploration on the same track and the results were used to select the hyperparameters for each of the algorithms used. In the second one the algorithm has been tested on a technically more complex track, while in the last part they evaluated the impact of adding future information of the track by adding a look ahead curvature and training in all three tracks.


\subsection{Imitation Learning in Videogames}

A promising approach in building human-like artificial players is what is commonly known as Imitation Learning \cite{imit1, imit2, il}, or Learning from Demonstration (LfD) \cite{lfd}, which consists in integrating some human expertise in the algorithm.
Some of the techniques employed in IL can be employed in RL algorithms to accelerate the learning process.


A popular algorithm called DAGGER \cite{dagger} provides an easy way for incorporating human experience. However, a human expert should always be available to provide feedback which is unrealistc in practice. 

Presented in \cite{dqlfd}, Deep Q-learning from Demonstrations (DQfD) vastly accelerates DQN by pretraining an initial behavior network, and also introducing a supervised loss and a L2 regularization loss when training the target network.

In \cite{drlhp} the authors considered rewards computed by a predictor trained with human feedback rather than from  interactions with the environment. This method separates the goal learning from the behavior learning by training a reward predictor with non-expert human preferences, and the behavior of the agent is improved accordingly.

\subsection{Imitation Learning in Racing Videogames}

Togelius and colleagues focused on the learning of a specific driving style using a simple 2D car simulator \cite{imit3}, \cite{imit4} and TORCS \cite{imit5}. In \cite{imit3}, the authors applied supervised learning (more precisely, neural networks and k-nearest neighbor \cite{knn}) to model driving styles directly, that is, by building models that could accurately predict the player actions based on the current game state. However, direct modeling resulted limited performance, accordingly, they moved to indirect modeling and used a genetic algorithm to evolve a controller with a driving style similar to a target human player. In \cite{imit4}, the approach based on indirect modeling was improved by using a different fitness function while later, in \cite{imit5}, it was extended by introducing a multi-objective evolutionary algorithm to evolve controllers which could be both robust (in that, they never run out of the track) and could demonstrate a driving style similar to one recorded from a user (in that, they accurately predict the user actions).


In  \cite{cardamone}, Cardamone, Loiacono and Lanzi applied supervised learning to develop car controllers in TORCS from the logs collected from other driver, and developed controllers capable of driving in non-trivial tracks reaching a performance that is in some cases 15\% lower than that of the fastest bot available in
the simulator (a good result if considered that supervised learning approaches generally yeld poor performances).
They considered two representations of the current state of the car:
\begin{itemize}
\item The first sensory representation is based on a rangefinder inputs usually employed in simulated car racing competitions.
The rangefinder casts an array of beams, with a range of 100m, around the car and returns the distance from the car to the track edge;
\item  A high-level, qualitative, representation involving basic lookahead information about the track in front of the car, based on a  novel sensor model, called lookahead sensor (see Figure~\ref{fig:cardamone2}). This representation is inspired by the behavior of human drivers whose decisions are usually based on high-level information about the current speed and trajectory and on the shape for the track ahead. 
For this purpose, the next $h$ meters of the track ahead are considered and divided into segments of the same length, and the lookahead sensor returns the bending radius for each segment: a rectilinear segment corresponds to a $0$; a positive radius corresponds to a right turn; while a negative radius corresponds to a left turn. 
\end{itemize} 

\begin{figure}
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/cardamone2}
  \caption{From \cite{cardamone}. Lookahead Sensor.}
   \label{fig:cardamone2}
\end{figure}
\begin{figure}
 \centering
  \captionsetup{width=10cm}
  \includegraphics[width=10cm]{./img/cardamone}
  \caption{From \cite{cardamone}. Trajectories comparison in track G-track-1: bot Inferno (blue line), 8 segments lookahead (red line), 16 segments lookahead (black line).}
  \label{fig:cardamone}
\end{figure}

Regarding the control phase, rather then acting on steer, throttle and break pedal, they propose  to control the car at a higher level. They try to learn the trajectories and the speeds along the track rather than low level commands.
In other words, instead of predicting the typical low-level actions on the car actuators available in TORCS, their approach is to predict a target speed and the car position with respect to the track axis.
A trajectory can be represented as the distance from the track axis in each segment of the track. So the output variables of the controller are the speed and the axis distance in the current segment.
The control is realized by means of three simple algorithms:
\begin{itemize}
\item Algorithm 1 computes the steer value: it describes the simple policy that controls the steering command to reach the target distance from the track axis. First, the wheel is aligned to the car axis and then a correction is applied to reduce the error;
\item Algorithms 2 computes the acceleration value;
\item Algorithms 3 computes the brake value. 
Algorithms 2 and 3 work this way: simply accelerate at full speed when the current speed is low and brake when is too high. 
\end{itemize}
A soft transition is finally applied between brake and accelerator when the actual speed is near to the target speed. This way, switching from full acceleration to full brake is avoided, resulting in no fluctuations around the target speed.

 
Finally, as concerns the learning phase, they considered two supervised learning methods, k-nearest neighbor classifiers \cite{knn} and multi-layer neural networks \cite{neat}, and applied them to compute a model mapping input sensors to actions which could imitate the behavior of an observed driver.
The model was then deployed to a driver controller that would map the high-level actions predicted by the models to low-level actions of the usual TORCS effectors
using the algorithms discussed in the previous section.

\textbf{KNN.} k-nearest neighbour does not involve any training in that the collected data represent the model. Accordingly, it has been directly applied during the evaluation, i.e. during a race using TORCS. At each game tic, the logged data are searched to find the k most similar instances to the current sensory input using either the typical rangefinder representation or the new lookahead representation. The k similar instances are selected and the corresponding outputs variable are averaged to predict the target value. The similarity measure used is simply the Manhattan distance among instances;
\textbf{Neural Networks.} To avoid the issue of selecting a particular neural network structure, they applied Neuroevolution with Augmenting Topology (NEAT) \cite{neat} which applies a genetic algorithm to evolve both the weights and the topology of a neural network. They evolved two separate neural networks, one to predict the target speed and one to predict the target position for a given input configuration. The fitness was defined as the prediction error computed as the total sum of the absolute error between the true value and the predicted value, for each instance in the training data. 
Their experiment consisted in running the Inferno bot on three tracks: G-track-1, a simple track; Wheel-1, a more difficult track with many fast turns; and Aalborg, a difficult track with many slow sharp turns. They logged the data from the rangefinders and from the lookahead sensors, together with the car distance from the track axis and the car speed which we use
as outputs.
The learned trajectories are shown in Figure~\ref{fig:cardamone}.




This paper \cite{cinesi} aims at improving the achievements of DQfD \cite{dqlfd} in integrating reinforcement learning with human demonstrations using TORCS.
Their goal is to bring experts demonstrations together with reinforcement learning, try to let the agent learn an appropriate policy by interacting with the environment on itself, and learn from expert’s demonstrations at the same time.
Their contribute develops in three parts:
\begin{itemize}
\item They shift from the discrete action domain considered in \cite{dqlfd} to a continuous action domain.
In order to determine the similarity between actions, they approximately evaluate the two considered actions by their mean squared error.  As DQN uses a greedy way to find the optimal policy, and the actions are finite discrete values, it is easy to calculate the supervised error for DQfD. However, in continuous action domain, the greedy policy is inapplicable, so they try to use the output of actor network directly for supervision. They combine a supervised loss with the original TD-loss to update the parameters of the critic network, and control the weighting between the losses by a given parameter;
\item They adjust the Experience Replay of DQN and DDPG by constructing an integrated replay buffer, which improves DDPG stability.  In DQN, the authors use a replay buffer to store the transitions generated by interacting with the environment, and randomly sample batches for training. In DQfD, two replay buffers are employed to store the self-generated data and demonstrator’s data respectively. Training data is sampled from these two buffers by a certain proportion. In this paper, the authors added another buffer to store the self-generated training data with good performances. 
For the initlia episodes, which usually do not contain enough good data, they instead use another buffer to collect the best transitions in every training episode, and substitute for the good performance buffer temporally. 
Their claim is that this procedure may improve the performance and stability of training;
\item  They attempt to learn human preferences without contradicting to the overall target of the task. Most RL methods have only an overall goal, but they do not consider the human behavior in realizing the goal, which can be reflected from the slightly differences between human personal preferences in completing the same task. They tackle this problem by recording demonstrations with human preferences and define specific optimization objectives to ensure the consistence with human preferences. To do that, they use demonstration data as the critic network’s inputs. The actor network yields an action, which must be similar enough with the demonstrator’s one. Meanwhile, this action needs to satisfy the Bellman Equation and to yield higher rewards when interacting with the environment. So, they use a combined loss to update the critic network’s parameters.
\end{itemize}
Their claim is that their method not only masters the preferences in choosing the lane, but also outperforms human expert demonstration in average reward.



Our work takes place in this context of imitation learning integrated with reinforcement learning applied to autonomous driving in TORCS without using sensory information. In the next section, we illustrate the software architecture and thereafter we describe techniques we used. 
