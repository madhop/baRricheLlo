\chapter{State of the Art}
\label{State of the Art}
\thispagestyle{empty}



The objective of this chapter is to go into detail about key aspects of reinforcement learning, the core topic of this thesis. In particular, we will describe the concept of Decision Markov Process, the experiment environment, the concept of state, actions, and reward function. Eventually, we will give insight about the algorithms we used in this project: Fitted Q-Iteration, and Deep Deterministic Policy Gradient. 
Then, in the following section, we will show some successful application on reinforcement learning to autonomous driving, videogames, and in particular racing games.


\section{Theoretical Background}

\subsection{Reinforcement Learning}
Together with Supervised Learning and Unsupervised Learning, Reinforcement Learning is a branch of Machine Learning, the discipline which studies the way a computer program can learn from experience and improve its performance at a specified task. Whereas Supervised Learning algorithms are based on inductiove inference where the model is tipically trained using labelled data to perform classification or regression, and Unsupervised Learning exploits techniques of density estimation and clustering applied to unlabelled data, in the Reinforcement Learning paradigm an autonomous agent learns to improve its performance at an assigned task by interacting with its environment.
Tipically, Reinforcement Learning agents are not explicitly taught how to act by an expert. Instead, the agent is free to explore the environment in which it lives, and its performance is evaluated by a reward function. Its goal is to maximize the reward by acting accordingly. The resulting reward results from the sum of the reward gained at every single step in its exploration process: for each state, the agent chooses an action to take and receives a reward based on the usefulness of its decision. Eventually, the agent learns the way to obtain the highest reward by exploting knowledge learned about the expected utility of different state-action pairs. The challenge of Reinforcement Learning is to design the best tradeoff between exploration and exploitation, that is the capability of an agent of use its knowledge to obtain high rewards and, by contrast, being capable of exploring new possibilities in such a way not to remain stuck in a local optimum. Intuitively, the exploration process is high at the first iterations of the learning process, while it should decrease with accumulating knowledge.

\subsubsection{Markov Decision Process}
The Reinforcement Learning problem can be formalized as a Markov Decision Process. A MDP is a tuple composed by:
\begin{itemize}
  \item A finite set of states \(D\): it encompasses every possible state of the process
  \item A finite set of actions \(A\): it represents the possible action the agent can take at any moment
  	\item A reward function \(r = \psi(s_t,a_t,s_{t+1})\), which represents the reward given to the agent at state \(s_t\) taking the action \(a_t\) landing in the state \(s_{t+1}\)
  	\item A transition probability model \(T(s_t,a_t,s_{t+1}) = p(s_{t+1}|s_t,a_t)\), which indicates the probability of landing in a state \(s_{t+1}\) being in a state \(s_t\) taking an action \(a_t\)
\end{itemize}

This provides the framework in which Reinforcement Learning operates: its goal is to find a policy \(\pi\) which specifies the actions to take in each state in order to maximize the reward over time i.e. fulfill the task.
Reinforcement learning can solve Markov decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration. In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states.
Reinforcement Learning operates under the Markov Assumption, under which the current state represents all the information needed to take an action, regardless from the past states and actions.
In other words: "The future is independent of the past given the present"

\subsubsection{Environment}
The environment is the mathematical representation of the world in which the agent operates. It reflects the set of features of a machine learning problem, which could be numerical (such as e.g. coordinates, velocities, acceleration) or raw (such as images or signals). On the representation of the environment depends the choice of the algorithm suitable to perform a learning process.

\subsubsection{Reward}
At each timestep, the agent receives a reward by the reinforcement learning algorithm, accordingly to the usefulness of action taken.
At the end of an episode (a finite sequence of states) the return is tipically computed as \[R_t = \sum^{T}_{k=0}\gamma^k r_t + k +1\] where \(\gamma\) represents a discount factor comprised between 0 and 1, which stands for a learning paramenter that influences how the agent considers furutre or present reward. A small \(\gamma\) results in a higher consideration of present rewards than the future (often caled "myopic" evaluation), while with a \(\gamma\) near to 1, the agent will consider equally the reward coming from the present either from the future (also called "far-sighted" evaluation). The role of the discount factor resides in the fact that, besides being mathematically convenient, it avoids infinite returns in cyclic Markov Process, and it's capable of addressing a non-fully represented uncertainty about the future.

\subsubsection{Policy}Ã¬

\subsection{Deep Reinforcement Learning}

\subsubsection{Fitted Q-Iteration}

\subsubsection{Deep Deterministic Policy Gradient}





















- FQI

	When the state and action spaces are finite and small enough, the Q-function can be represented in tabular form, and its approximation (in batch and in on-line mode) as well as the control policy derivation are straightforward. However, when dealing with continuous or very large discrete state and/or action spaces, the Q-function cannot be represented anymore by a table with one entry for each state-action pair. Moreover, in the context of reinforcement learning an approximation of the Q-function all over the state-action space must be determined from finite and generally very sparse sets of four-tuples.
To overcome this generalization problem, a particularly attractive framework is the one used by Ormoneit and Sen (2002) which applies the idea of fitted value iteration (Gordon, 1999) to kernel-based reinforcement learning, and reformulates the Q-function determination problem as a sequence of kernel-based regression problems. Actually, this framework makes it possible to take full advantage in the context of reinforcement learning of the generalization capabilities of any regression algorithm, and this contrary to stochastic approximation algorithms (Sutton, 1988; Tsitsiklis, 1994) which can only use parametric function approximators (for example, linear combinations of feature vectors or neural networks). In the rest of this paper we will call this framework the fitted Q iteration
algorithm so as to stress the fact that it allows to fit (using a set of four-tuples) any (parametric or non-parametric) approximation architecture to the Q-function.
The fitted Q iteration algorithm is a batch mode reinforcement learning algorithm which yields an approximation of the Q-function corresponding to an infinite horizon optimal control problem with discounted rewards, by iteratively extending the optimization horizon (Ernst et al., 2003).
At each step this algorithm may use the full set of four-tuples gathered from observation of the system together with the function computed at the previous step to determine a new training set which is used by a supervised learning (regression) method to compute the next function of the sequence. It produces a sequence of Q N -functions, approximations of the Q N -functions defined by Eqn (5). --> mettere algoritmo a pag 6 di tree based batch reinforcement learning

- extra trees
	Besides Tree Bagging, several other methods to build tree ensembles have been proposed that often improve the accuracy with respect to Tree Bagging (e.g. Random Forests, Breiman, 2001). In this paper, we evaluate our recently developed algorithm that we call "Extra-Trees", for extremely randomized trees (Geurts et al., 2004). Like Tree Bagging, this algorithm works by building several (M) trees. However, contrary to Tree Bagging which uses the standard CART algorithm to derive the trees from a bootstrap sample, in the case of Extra-Trees, each tree is built from the complete original training set. To determine a test at a node, this algorithm selects K cut-directions at random and for each cut-direction, a cut-point at random. It then computes a score for each of the K tests and
chooses among these K tests the one that maximizes the score. Again, the algorithm stops splitting a node when the number of elements in this node is less than a parameter n min . Three parameters are associated to this algorithm: the number M of trees to build, the number K of candidate tests at each node and the minimal leaf size n min . The detailed tree building procedure is given in Appendix A.
	
	- Double learning
	- Double FQI

- DDPG
	- DPG
	



- background:
pag 3-4 cinesi



Presented in [16], Deep Q-learning from Demonstrations
(DQfD) vastly accelerates DQN by pretraining an initial
behavior network, and also introducing a supervised loss and
a L2 regularization loss when training the target network.

A combination of the advantages of both, the speed of the Riccati controller and the generality of MPC, can be achieved by finding a function that maps state values to
control variables, e. g., by training a deep neural network. Such a model could, for example, be learned supervised, as done for PILOTNET, or by reinforcement learning. The latter in particular led to excellent results in the training of such agents for controlling real-world systems such as robots or helicopters.
Recent work also shows promising applications of reinforcement learning for autonomous driving by making strategic decisions. 
Autonomous driving tasks where RL could be applied include: controller optimization, path planning and trajectory optimization, motion planning and dynamic path planning, development of high-level driving policies for complex navigation tasks, scenario-based policy learning for highways, intersections, merges and splits, reward learning with inverse reinforcement learning from expert data for intent prediction for traffic actors such as pedestrian, vehicles and finally learning of policies that ensures safety and perform risk estimation. Further, it turns out to be suitable in contexts of autonomous racing: the driverless racer could learn a policy that is able to outperform the performance of a human driver, or a policy taught by experts.





\section{Related Works}









In this section we'll explore some of the applications of Reinforcement Learning made by the scientific community focusing on Reinforcement Learning from demonstrations, which aims at integrating automatic learning with human experience.



\subsection{Reinforcement Learning in Videogames}
- atari
-alphago


By learning a deep convolutional neural network
to approximate the Q-function, Mnih et al. successfully
construct a DRL framework called Deep Q-Network (DQN)
which plays Atari games in human level

\subsubsection{Reinforcement Learning in Racing Games}



- introduzione al problema della traiettoria
Finding a racing line that allows to achieve a competitive lap-time is a key problem in real-world car racing as well as in the development of non-player characters for a commercial racing game.
The optimal racing line is defined as the line to follow to achieve the best lap-time possible on a given track with a given car. In general, finding the optimal racing line is
a common problem in real-world car racing as well as in the development of commercial racing games. The optimal racing line is the path that a driver should follow to complete a lap on a given track in the smallest amount of time possible. As the lap-time depends both on the distance raced and on the average racing speed, finding the optimal racing involves two different sub-problems: racing the shortest distance possible and racing as fast
as possible along the track.

- giochi commerciali: 
	colin mcrae

- papers:

- tecniche classiche
During the years, different attempts to achieve time-optimal racing, evolving together with technology. Controlling a self-driving car can be done with a planner, if the optimal trajectory is a priori computed, or by a controller, such as a PID, or a MPC, or with Machine Learning. In this section, we provide an overview of such techniques. 


- Evolving the Optimal Racing Line in a High-End Racing Game
This paper shows how to encode a racing line by a set of connected BÃ©zier curves, such that each gene defines a small portion of the racing line. Therefore, the evolution is responsible of the entire design of the racing line. In addition, the authors compare two different methods to evaluate the evolved racing line; the first one is based on testing the evolved racing lines in a racing simulator; the second one consists of estimating the performance of a racing line through a computational model.
	
- Ahura: A Heuristic-Based Racer for the Open Racing Car Simulator
This work proposes a controller called Ahura for TORCS. The controller uses five modules: steer controller, speed controller, opponent manager (it creates a map of opponents around  and finds the vacant slot to overtake), dynamic adjuster (friction, bumps), stuck manager (uses the idea proposed in to control the vehicle when it is out of the track or it has stuck somewhere.).
There are 23 parameters in Ahura that need to be determined:
1) eight parameters for the steer controller 
2) ten parameters for the speed controller 
3) five parameters for the opponent manager
They were determined by using the optimization algorithm CMA-ES: it has a good performance in continuous space, works with nonlinear systems, no constraint handling technique is required, and it is appropriate for nonseparable search spaces.


- Model predictive control Towards Time-Optimal Race Car Driving using Nonlinear MPC in
Real-Time
Model Predictive Control (MPC) is a multivariable control algorithm that uses:
an internal dynamic model of the process
a cost function J over the receding horizon
an optimization algorithm minimizing the cost function J using the control input u
Existing advanced control based attempts to minimum-time driving usually provide only offline open-loop solutions. . In this paper, instead, we directly use a one-level approach, solving the nonlinear MPC problem with an economic cost function in real-time. The tight real-time bounds imposed on computational times make it necessary to
reformulate the problem so as to allow for the use of efficient algorithms.
It provides real-world experimental results of the proposed nonlinear MPC scheme from a miniature race-car setup that is detailed in the paper. 

- machine learning
Machine learning allows an agent to learn in an automatic fashion. This avoids the need to know apriori different aspects of the problem, such as the dynamic model of the vehicle, or the map of the environment. Moreover, 

The following two papers give an idea of application of reinforcement learning to autonomous driving, without posing the attention about racing, while the next ones are focused specifically about racing.

- Controlling an Autonomous Vehicle with Deep Reinforcement Learning
This paper https://arxiv.org/pdf/1909.12153.pdf shows an example of end-to-end reinforcement learning application to autonomous drive, using a proximal policy optimization algorithms by means of a neural network which maps the state to the controls. It learns two different policies: the driver and the stopper, and the reward is the squared proximity to a target position. The work aims at realizing the autonomous exploration of a parking lot based on deep reinforcement learning. It describes how a policy is trained to compute sophisticated control commands which depend on an estimate of the current vehicle state. This is done by designing an appropriate Markov decision process and a corresponding proximal policy optimization learning algorithm. For that purpose a simulated environment is used for data generation. Here, information about the vehicle's surrounding are measured by, e. g., laser scanners and are further extended by a rough knowledge about the geometry of the drivable area.
Here, the state is composed by: steering wheel angle and longitudinal acceleration; the state transition is made by single-track-models. The vehicle is assumed to have only one wheel at the front and back respectively, each centered between the real ones. Finally, two reward function are used in order to train a driver and a stopper function.




- CARMA: A Deep Reinforcement Learning Approach to Autonomous Driving
In this paper they experiment different setups of Deep Neural Networks, such as CNN and RNN as functions approximator of a reinfocement learning agent.
They formulate the state-space as follows: At each timestep, the agent receives an image I t of the environment, an estimate of the agent's speed, an estimate of the agent's distance to the center of the road, and an estimate of the angle between the agent's forward vector and the center of the road. At each timestep t, the agent can perform a discrete action: acceleration a = (Brake, DoNothing, Accelerate) and steer t = (TurnLef, DoNothing, TurnRight). Thus,there are a total of 9 possible actions the car can take at each timestep.
Then, they experimented different modeling approaches based on Q-Learning, involving CNN and RNN agents.


- Simulation-based reinforcement learning for autonomous driving:
This paper applies via reinforcement throughe Proximal Policy Optimization (PPO) with a contin-ous action space operating on multiple tensors of multiple shapes: a front camera, a high-level navigation command, car speed, and car acceleration. Thus, they adopted a custom policy that operates on multiple input tensors coming from one observation. They operated within the CARLA simulator. Their policy controls only the steering, while throttle is controlled via a PID controller with the speed set to a constant.
They modeled continuous actions with the Gaussian distribution.
The agent receives as its observation an RGB image from a single front camera from which extract semantic segmentation and car metrics such as speed and acceleration. The agent is also provided with high-level navigation command. 

In racing tasks, due to the complexity of the problem, the most successful approaches has been proved to be the ones that incorporate an automatic learning process with human expertise.

- Learning Drivers for TORCS through Imitation Using Supervised Methods	
In this work, Cardamone, Loiacono and Lanzi applied supervised learning to develop car controllers for The Open Car Race Simulator from the logs collected from other drivers. 
They considered two representations of the current state of the car: the set of rangefinder inputs usually employed in simulated car racing competitions, and a high-level, qualitative, representation involving basic lookahead information about the track in front of the car. Instead of predicting the typical low-level actions on the car actuators available in TORCS (namely, the steering wheel, the gas pedal, the brake pedal and the gear change), their approach predicts a target speed and the car position with respect to the track axis.
They considered two supervised learning methods, multi-layer neural networks and k-nearest neighbor classifiers, and applied them to compute a model mapping input sensors to actions which could imitate the behavior of an observed driver.

- In this paper \cite{cinesi}, the authors addresses an issue which is similar to ours: they aim at integrate a reinforcement learning algorithm with human expertise. Their claim is that this process could help accelerate the exploration and increase learning agent's stability.
Their framework is a continuous reinforcement learning setting, in which they integrate DDPG with human demonstrations.
To achieve that, they formulate a loss function to optimize which combines the experience gathered from the environment and by the demonstrations. In such a way, with respect to a state, the agent should pick an action wich is consisent with the human behavior.

- Finn et al. proposed an Inverse Reinforcement Learning \cite{inverse} approach to integrate human demonstrations with reinforcement learning. They applied the algorithm to optimal robotic control.



- Our work proposed different approaches to solve time-optimal racing problem in the framework of reinforcement learning. ampliare, parlare di torcs